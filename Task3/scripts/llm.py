# -*- coding: utf-8 -*-
"""LLM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IPntOlbNPcefOufWFB2sE00Wj6FQ_0sH
"""

pip install transformers datasets

pip install peft

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model

# Load dataset
dataset = load_dataset("Cynaptics/persona-chat")  # Replace with your dataset name or path

# Check for train-validation split and create one if necessary
if "validation" not in dataset:
    dataset = dataset["train"].train_test_split(test_size=0.1)

# Load tokenizer and model
model_name = "gpt2"  # Replace with your preferred pre-trained model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Ensure padding token is set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Apply LoRA configuration
lora_config = LoraConfig(
    task_type="CAUSAL_LM",  # Causal language modeling
    r=4,  # Rank of the LoRA layers
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,  # Dropout probability
    target_modules=["c_attn", "c_proj"],  # Target Conv1D layers in GPT-2
)

model = get_peft_model(model, lora_config)

# Preprocessing function
def preprocess_function(examples):
    inputs = []
    labels = []
    for persona, dialogue, reference in zip(
        examples["persona_b"], examples["dialogue"], examples["reference"]
    ):
        prompt = f"""
        Persona of Person B: {persona}

        Instruct: Person A and Person B are now having a conversation.
        Following the conversation below, write a response that Person B would say based on the above Persona information.
        Please carefully consider the flow and context of the conversation below, and use Person B's Persona information appropriately.

        {dialogue}

        Output:
        """
        inputs.append(prompt)
        labels.append(reference)

    model_inputs = tokenizer(inputs, truncation=True, padding="max_length", max_length=512)
    with tokenizer.as_target_tokenizer():
        model_labels = tokenizer(labels, truncation=True, padding="max_length", max_length=512)

    model_inputs["labels"] = model_labels["input_ids"]
    return model_inputs

# Tokenize dataset
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=5e-4,  # Higher learning rate for LoRA
    per_device_train_batch_size=1,
    gradient_accumulation_steps=5,
    num_train_epochs=1,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    save_steps=500,  # Save the model periodically
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],  # Use "test" as the validation dataset after splitting
    tokenizer=tokenizer,
)

# Train the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained("./trained_model_lora")
tokenizer.save_pretrained("./trained_model_lora")

# Generate outputs
def generate_response(persona, dialogue):
    input_text = f"""
    Persona of Person B: {persona}

    Instruct: Person A and Person B are now having a conversation.
    Following the conversation below, write a response that Person B would say based on the above Persona information.
    Please carefully consider the flow and context of the conversation below, and use Person B's Persona information appropriately.

    {dialogue}

    Output:
    """
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    output_ids = model.generate(input_ids, max_length=200, num_return_sequences=1, do_sample=True)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Test the model with a sample input
persona = "My name is David and I'm a 35-year-old math teacher. I like to hike and spend time in nature. I'm married with two kids."
dialogue = "Persona A: Morning! I think I saw you at the parent meeting, what's your name?"

print("Generated Response:")
print(generate_response(persona, dialogue))

